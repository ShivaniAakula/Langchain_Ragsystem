{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVf6uhlg_Sxp"
   },
   "source": [
    "# **Build a RAG System on “Leave No Context Behind” Paper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wo81NNPm_Lgh",
    "outputId": "d74e28f4-fe02-4ae8-87c6-764ea1d77aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pypdf) (4.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RBdgRAuDCRiH",
    "outputId": "1df09c9f-2549-449e-9aff-4b9c0af8f5d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_google_genai\n",
      "  Using cached langchain_google_genai-1.0.2-py3-none-any.whl (28 kB)\n",
      "Collecting google-generativeai<0.6.0,>=0.5.0\n",
      "  Using cached google_generativeai-0.5.2-py3-none-any.whl (146 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.27\n",
      "  Using cached langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
      "Collecting google-api-python-client\n",
      "  Using cached google_api_python_client-2.127.0-py2.py3-none-any.whl (12.7 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (4.64.1)\n",
      "Collecting google-ai-generativelanguage==0.6.2\n",
      "  Using cached google_ai_generativelanguage-0.6.2-py3-none-any.whl (664 kB)\n",
      "Collecting google-auth>=2.15.0\n",
      "  Using cached google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (4.9.0)\n",
      "Collecting google-api-core\n",
      "  Using cached google_api_core-2.18.0-py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (4.25.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (2.7.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.6.2->google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (1.23.0)\n",
      "Collecting langsmith<0.2.0,>=0.1.0\n",
      "  Using cached langsmith-0.1.50-py3-none-any.whl (115 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-core<0.2,>=0.1.27->langchain_google_genai) (6.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-core<0.2,>=0.1.27->langchain_google_genai) (8.2.3)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-core<0.2,>=0.1.27->langchain_google_genai) (23.2)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (5.3.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2,>=0.1.27->langchain_google_genai) (2.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.27->langchain_google_genai) (2.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.27->langchain_google_genai) (3.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (2.18.1)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Using cached googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (4.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (0.4.6)\n",
      "Collecting grpcio<2.0dev,>=1.33.2\n",
      "  Using cached grpcio-1.62.2-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "  Using cached grpcio_status-1.62.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.6.0,>=0.5.0->langchain_google_genai) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.27->langchain_google_genai) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.27->langchain_google_genai) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.27->langchain_google_genai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2,>=0.1.27->langchain_google_genai) (3.4)\n",
      "Installing collected packages: jsonpatch, httplib2, grpcio, googleapis-common-protos, grpcio-status, google-auth, langsmith, google-auth-httplib2, google-api-core, langchain-core, google-api-python-client, google-ai-generativelanguage, google-generativeai, langchain_google_genai\n",
      "  Attempting uninstall: jsonpatch\n",
      "    Found existing installation: jsonpatch 1.32\n",
      "    Uninstalling jsonpatch-1.32:\n",
      "      Successfully uninstalled jsonpatch-1.32\n",
      "Successfully installed google-ai-generativelanguage-0.6.2 google-api-core-2.18.0 google-api-python-client-2.127.0 google-auth-2.29.0 google-auth-httplib2-0.2.0 google-generativeai-0.5.2 googleapis-common-protos-1.63.0 grpcio-1.62.2 grpcio-status-1.62.2 httplib2-0.22.0 jsonpatch-1.33 langchain-core-0.1.45 langchain_google_genai-1.0.2 langsmith-0.1.50\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "XFzPJDU5CV7Y",
    "outputId": "2a9551ca-f599-46a1-9c68-7f66c6b20662"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Setup API Key\n",
    "f = open(r\"C:\\Users\\HP\\OneDrive\\Desktop\\projects\\Search_Engine\\keys\\Lang_Chain_Key.txt\")\n",
    "GOOGLE_API_KEY = f.read()\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(google_api_key=GOOGLE_API_KEY, model=\"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzCJ9vMQ_fgL"
   },
   "source": [
    "# Loading the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "X565FTL9_iv0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "     -------------------------------------- 817.7/817.7 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.32\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (2.7.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (0.1.50)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (0.1.45)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (1.33)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (2.28.1)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-win_amd64.whl (370 kB)\n",
      "     -------------------------------------- 370.7/370.7 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "     -------------------------------------- 50.4/50.4 kB 287.1 kB/s eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
      "     -------------------------------------- 76.4/76.4 kB 384.7 kB/s eta 0:00:00\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "     -------------------------------------- 49.4/49.4 kB 358.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Installing collected packages: typing-inspect, multidict, marshmallow, frozenlist, async-timeout, yarl, dataclasses-json, aiosignal, aiohttp, langchain-text-splitters, langchain-community, langchain\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 dataclasses-json-0.6.4 frozenlist-1.4.1 langchain-0.1.16 langchain-community-0.0.34 langchain-text-splitters-0.0.1 marshmallow-3.21.1 multidict-6.0.5 typing-inspect-0.9.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "y2bQoMUX_nSA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Preprint. Under review.\\nLeave No Context Behind:\\nEfficient Infinite Context Transformers with Infini-attention\\nTsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal\\nGoogle\\ntsendsuren@google.com\\nAbstract\\nThis work introduces an efficient method to scale Transformer-based Large\\nLanguage Models (LLMs) to infinitely long inputs with bounded memory\\nand computation. A key component in our proposed approach is a new at-\\ntention technique dubbed Infini-attention. The Infini-attention incorporates\\na compressive memory into the vanilla attention mechanism and builds\\nin both masked local attention and long-term linear attention mechanisms\\nin a single Transformer block. We demonstrate the effectiveness of our\\napproach on long-context language modeling benchmarks, 1M sequence\\nlength passkey context block retrieval and 500K length book summarization\\ntasks with 1B and 8B LLMs. Our approach introduces minimal bounded\\nmemory parameters and enables fast streaming inference for LLMs.\\n1 Introduction\\nMemory serves as a cornerstone of intelligence, as it enables efficient computations tailored\\nto specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based\\nLLMs (Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024) have\\na constrained context-dependent memory, due to the nature of the attention mechanism.\\nUpdate \\nVVConcat Concat \\nQ V\\nQ V\\nQs{KV}sCompressive memory & \\nLinear attention Causal scaled dot-product \\nattention & PE Linear \\nprojection \\n{KV}s-1Retrieve \\nFigure 1: Infini-attention has an addi-\\ntional compressive memory with linear\\nattention for processing infinitely long\\ncontexts. {KV}s−1and{KV}sare atten-\\ntion key and values for current and previ-\\nous input segments, respectively and Qs\\nthe attention queries. PE denotes position\\nembeddings.The attention mechanism in Transformers ex-\\nhibits quadratic complexity in both memory\\nfootprint and computation time. For example,\\nthe attention Key-Value (KV) states have 3TB\\nmemory footprint for a 500B model with batch\\nsize 512 and context length 2048 (Pope et al.,\\n2023). Indeed, scaling LLMs to longer sequences\\n(i.e. 1M tokens) is challenging with the standard\\nTransformer architectures and serving longer\\nand longer context models becomes costly finan-\\ncially.\\nCompressive memory systems promise to be\\nmore scalable and efficient than the attention\\nmechanism for extremely long sequences (Kan-\\nerva, 1988; Munkhdalai et al., 2019). Instead\\nof using an array that grows with the input se-\\nquence length, a compressive memory primarily\\nmaintains a fixed number of parameters to store\\nand recall information with a bounded storage\\nand computation costs. In the compressive mem-\\nory, new information is added to the memory\\nby changing its parameters with an objective\\nthat this information can be recovered back later\\non. However, the LLMs in their current state\\nhave yet to see an effective, practical compres-\\nsive memory technique that balances simplicity along with quality.\\n1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024', metadata={'source': 'C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\projects\\\\Search_Engine\\\\2404.07143.pdf', 'page': 0}),\n",
       " Document(page_content='Preprint. Under review.\\nIn this work, we introduce a novel approach that enables Transformer LLMs to effectively\\nprocess infinitely long inputs with bounded memory footprint and computation. A key\\ncomponent in our proposed approach is a new attention technique dubbed Infini-attention\\n(Figure 1). The Infini-attention incorporates a compressive memory into the vanilla attention\\nmechanism (Bahdanau et al., 2014; Vaswani et al., 2017) and builds in both masked local\\nattention and long-term linear attention mechanisms in a single Transformer block.\\nSuch a subtle but critical modification to the Transformer attention layer enables a natural\\nextension of existing LLMs to infinitely long contexts via continual pre-training and fine-\\ntuning.\\nOur Infini-attention reuses all the key, value and query states of the standard attention\\ncomputation for long-term memory consolidation and retrieval. We store old KV states of\\nthe attention in the compressive memory, instead of discarding them like in the standard\\nattention mechanism. We then retrieve the values from the memory by using the attention\\nquery states when processing subsequent sequences. To compute the final contextual\\noutput, the Infini-attention aggregates the long-term memory-retrieved values and the local\\nattention contexts.\\nIn our experiments, we show that our approach outperforms baseline models on long-\\ncontext language modeling benchmarks while having 114x comprehension ratio in terms of\\nmemory size. The model achieves even better perplexity when trained with 100K sequence\\nlength. A 1B LLM naturally scales to 1M sequence length and solves the passkey retrieval\\ntask when injected with Infini-attention. Finally, we show that a 8B model with Infini-\\nattention reaches a new SOTA result on a 500K length book summarization task after\\ncontinual pre-training and task fine-tuning.\\nIn summary, our work makes the following contributions:\\n1.We introduce a practical and yet powerful attention mechanism – Infini-attention\\nwith long-term compressive memory and local causal attention for efficiently mod-\\neling both long and short-range contextual dependencies.\\n2.Infini-attention introduces minimal change to the standard scaled dot-product atten-\\ntion and supports plug-and-play continual pre-training and long-context adaptation\\nby design.\\n3.Our approach enables Transformer LLMs to scale to infinitely long context with a\\nbounded memory and compute resource by processing extremely long inputs in a\\nstreaming fashion.\\n2 Method\\nFigure 2 compares our model, Infini-Transformer, and Transformer-XL (Dai et al., 2019).\\nSimilar to Transformer-XL, Infini-Transformer operates on a sequence of segments. We\\ncompute the standard causal dot-product attention context within each segment. So the\\ndot-product attention computation is local in a sense that it covers a total Nnumber of\\ntokens of the current segment with index S(Nis the segment length).\\nThe local attention (Dai et al., 2019), however, discards the attention states of the previous\\nsegment when processing the next one. In Infini-Transformers, instead of leaving out the\\nold KV attention states, we propose to reuse them to maintain the entire context history\\nwith a compressive memory. So each attention layer of Infini-Transformers has both global\\ncompressive and local fine-grained states. We call such an efficient attention mechanism\\nInfini-attention, which is illustrated in Figure 1 and described formally in the following\\nsections.\\n2.1 Infini-attention\\nAs shown Figure 1, our Infini-attention computes both local and global context states and\\ncombine them for its output. Similar to multi-head attention (MHA), it maintains Hnumber\\n2', metadata={'source': 'C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\projects\\\\Search_Engine\\\\2404.07143.pdf', 'page': 1}),\n",
       " Document(page_content='Preprint. Under review.\\nSegment 1 Segment 2 Segment 3 \\nSegment 1 Segment 2 Segment 3 Transformer block: Infini-T ransformer \\nT ransformer-XL Compressive memory: \\nMemory update: \\nMemory retrieval: \\nEffective context: \\nInput segment: Segment 1 \\nFigure 2: Infini-Transformer (top) has an entire context history whereas Transformer-XL\\n(bottom) discards old contexts since it caches the KV states for the last segment only.\\nof parallel compressive memory per attention layer ( His the number of attention heads) in\\naddition to the dot-product attention.\\n2.1.1 Scaled Dot-product Attention\\nThe multi-head scaled dot-product attention (Vaswani et al., 2017), specially its self-attention\\nvariant (Munkhdalai et al., 2016; Cheng et al., 2016), has been the main building block in\\nLLMs. The MHA’s strong capability to model context-dependent dynamic computation and\\nits conveniences of temporal masking have been leveraged extensively in the autoregressive\\ngenerative models.\\nA single head in the vanilla MHA computes its attention context Adot∈I RN×dvaluefrom\\nsequence of input segments X∈I RN×dmodel as follows. First, it computes attention query,\\nkey, and value states:\\nK=XW K,V=XW Vand Q=XW Q. (1)\\nHere, WK∈I Rdmodel×dkey,WV∈I Rdmodel×dvalueand WQ∈I Rdmodel×dkeyare trainable projection\\nmatrices. Then, the attention context is calculated as a weighted average of all other values\\nas\\nAdot=softmax\\x12QKT\\n√dmodel\\x13\\nV. (2)\\nFor MHA, we compute Hnumber of attention context vectors for each sequence element\\nin parallel, concatenate them along the second dimension and then finally project the\\nconcatenated vector to the model space to obtain attention the output.\\n2.1.2 Compressive Memory\\nIn Infini-attention, instead of computing new memory entries for compressive memory, we\\nreuse the query, key and value states ( Q,Kand V) from the dot-product attention compu-\\ntation. The state sharing and reusing between the dot-product attention and compressive\\nmemory not only enables efficient plug-in-play long-context adaptation but also speeds up\\ntraining and inference. Similar to the prior work (Munkhdalai et al., 2019), our goal is to\\nstore bindings of key and value states in the compressive memory and retrieve by using the\\nquery vectors.\\n3', metadata={'source': 'C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\projects\\\\Search_Engine\\\\2404.07143.pdf', 'page': 2}),\n",
       " Document(page_content='Preprint. Under review.\\nWhile there are different forms of compressive memory proposed in the literature (Hop-\\nfield, 1982; Kanerva, 1988; Schlag et al., 2019; Munkhdalai et al., 2019), for simplicity and\\ncomputational efficiency, in this work we parameterize the memory with an associative\\nmatrix (Schlag et al., 2020). This approach further allows us to cast the memory update\\nand retrieval process as linear attention mechanism (Shen et al., 2018) and to leverage\\nstable training techniques from the related methods. Specially, we adopt the update rule\\nand retrieval mechanism by Katharopoulos et al. (2020) mainly due to its simplicity and\\ncompetitive performance.\\nMemory retrieval. In Infini-attention, we retrieve new content Amem∈I RN×dvaluefrom the\\nmemory Ms−1∈I Rdkey×dvalueby using the query Q∈I RN×dkeyas:\\nAmem=σ(Q)Ms−1\\nσ(Q)zs−1. (3)\\nHere, σand zs−1∈I Rdkeyare a nonlinear activation function and a normalization term,\\nrespectively. As the choice of the non-linearity and the norm method is crucial for training\\nstability, following Katharopoulos et al. (2020) we record a sum over all keys as the normal-\\nization term zs−1and use element-wise ELU + 1 as the activation function (Clevert et al.,\\n2015).\\nMemory update. Once the retrieval is done, we update the memory and the normalization\\nterm with the new KV entries and obtain the next states as\\nMs←Ms−1+σ(K)TVand zs←zs−1+N\\n∑\\nt=1σ(Kt). (4)\\nThe new memory states Msand zsare then passed to the next segment S+1, building in\\na recurrence in each attention layer. The right side term σ(K)TVin Eq. (4)is known as an\\nassociative binding operator (Smolensky, 1990; Hebb, 2005; Schlag et al., 2020).\\nInspired by the success of delta rule (Munkhdalai et al., 2019; Schlag et al., 2020; 2021),\\nwe have also incorporated it into our Infini-attention. The delta rule attempts a slightly\\nimproved memory update by first retrieving existing value entries and subtracting them\\nfrom the new values before applying the associative bindings as new update.\\nMs←Ms−1+σ(K)T(V−σ(K)Ms−1\\nσ(K)zs−1). (5)\\nThis update rule ( Linear +Delta ) leaves the associative matrix unmodified if the KV binding\\nalready exists in the memory while still tracking the same normalization term as the former\\none ( Linear ) for numerical stability.\\nLong-term context injection. We aggregate the local attention state Adotand memory\\nretrieved content Amemvia a learned gating scalar β:\\nA=sigmoid (β)⊙Amem+ (1−sigmoid (β))⊙Adot. (6)\\nThis adds only a single scalar value as training parameter per head while allowing a\\nlearnable trade-off between the long-term and local information flows in the model (Wu\\net al., 2022).\\nSimilar to the standard MHA, for the multi-head Infini-attention we compute Hnumber of\\ncontext states in parallel, and concatenate and project them for the final attention output\\nO∈I RN×dmodel:\\nO= [A1; . . .AH]WO (7)\\nwhere WO∈I RH×dvalue×dmodelis trainable weights.\\n2.2 Memory and Effective Context Window\\nOur Infini-Transformer enables an unbounded context window with a bounded memory\\nfootprint. To illustrate this, Table 1 lists the previous segment-level memory models with\\n4', metadata={'source': 'C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\projects\\\\Search_Engine\\\\2404.07143.pdf', 'page': 3}),\n",
       " Document(page_content='Preprint. Under review.\\nModel Memory (cache) footprint Context length Memory update Memory retrieval\\nTransformer-XL (dkey+dvalue)×H×N×l N ×l Discarded Dot-product attention\\nCompressive Transformer dmodel×(c+N)×l (c×r+N)×l Discarded Dot-product attention\\nMemorizing Transformers (dkey+dvalue)×H×N×S N ×S None kNN + dot-product attention\\nRMT dmodel×p×l×2 N×S Discarded Soft-prompt input\\nAutoCompressors dmodel×p×(m+1)×l N ×S Discarded Soft-prompt input\\nInfini-Transformers dkey×(dvalue+1)×H×l N ×S Incremental Linear attention\\nTable 1: Transformer models with segment-level memory are compared. For each model, the\\nmemory size and effective context length are defined in terms of their model parameters ( N:\\ninput segment length, S: the number of segments, l: the number of layers, H: the number\\nof attention heads, c: Compressive Transformer memory size, r: compression ratio, p: the\\nnumber of soft-prompt summary vectors and m: summary vector accumulation steps).\\ntheir context-memory footprint and effective context length defined in terms of model\\nparameters and input segment length. Infini-Transformer has a constant memory complexity\\nofdkey×dvalue+dkeyfor storing compressed context in Msand zsfor each head in single\\nlayer while for the other models, the complexity grows along with the sequence dimension\\n- the memory complexity depends either on the cache size for Transformer-XL (Dai et al.,\\n2019), Compressive Transformer (Rae et al., 2019) and Memorizing Transformers (Wu et al.,\\n2022) or on the soft-prompt size for RTM (Bulatov et al., 2022) and AutoCompressors (Ge\\net al., 2023).\\nEarly \\nlayers \\nAttention heads \\nFigure 3: There are two types of heads\\nemerged in Infini-attention after training: spe-\\ncialized heads with gating score near 0 or\\n1 and mixer heads with score close to 0.5.\\nThe specialized heads either process contex-\\ntual information via the local attention mech-\\nanism or retrieve from the compressive mem-\\nory whereas the mixer heads aggregate both\\ncurrent contextual information and long-term\\nmemory content together into single output.Transformer-XL computes attention over KV\\nstates cached from the last segment in addition\\nto the current states. Since this is done for each\\nlayer, Transformer-XL extends the context win-\\ndow from NtoN×ltokens with an additional\\nmemory footprint of (dkey+dvalue)×H×N×l.\\nCompressive Transformer adds a second cache\\nto Transformer-XL and stores compressed rep-\\nresentations of past segment activations. So it\\nextends the Transformer-XL’s context window\\nbyc×r×lbut still has a large context-memory\\ncomplexity. Taking the idea further, Memoriz-\\ning Transformers opt to store the entire KV states\\nas context for input sequences. Since the stor-\\nage becomes prohibitively expensive in this case,\\nthey restrict the contextual computation to a sin-\\ngle layer only. By utilizing a fast kNN retriever,\\nMemorizing Transformers then build a context\\nwindow covering the entire sequence history of\\nlength N×Sat an increased cost of storage. Our\\nexperiments show that Infini-Transformer LM\\ncan achieve more than 100x compression rate on\\ntop of Memorizing Transformers while further\\nimproving the perplexity score.\\nRMT and AutoCompressors allow for a poten-\\ntially infinite context length since they compress\\nthe input into summary vectors and then pass\\nthem as extra soft-prompt inputs for the subsequent segments. However, in practice the\\nsuccess of those techniques highly depends on the size of soft-prompt vectors. Namely, it\\nis necessary to increase the number of soft-prompt (summary) vectors to achieve a better\\nperformance with AutoCompressors (Chevalier et al., 2023) and with that, the memory and\\ncompute complexity grow quickly resulting in diminished efficiency. It was also observed in\\nAutoCompressors (Chevalier et al., 2023) that an efficient compression objective is needed\\nfor training such prompt compression techniques (Ge et al., 2023).\\n5', metadata={'source': 'C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\projects\\\\Search_Engine\\\\2404.07143.pdf', 'page': 4})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a document\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(r\"C:\\Users\\HP\\OneDrive\\Desktop\\projects\\Search_Engine\\2404.07143.pdf\")\n",
    "\n",
    "data = loader.load_and_split()\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqLQfxlA_phR"
   },
   "source": [
    "# Spliting the document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pQfXiY2j_t4J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 568, which is longer than the specified 500\n",
      "Created a chunk of size 506, which is longer than the specified 500\n",
      "Created a chunk of size 633, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "# Spliting the document into chunks\n",
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9dMs2uM_wR2"
   },
   "source": [
    "# Creating Chunk Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8Bo4-yBg_ytR"
   },
   "outputs": [],
   "source": [
    "# Creating Chunks Embedding\n",
    "# We are just loading OpenAIEmbeddings\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=GOOGLE_API_KEY, model=\"models/embedding-001\")\n",
    "\n",
    "# vectors = embeddings.embed_documents(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC6ddC_K_1DS"
   },
   "source": [
    "# Storing the chunks in vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadbNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading chromadb-0.5.0-py3-none-any.whl (526 kB)\n",
      "     ------------------------------------ 526.8/526.8 kB 786.3 kB/s eta 0:00:00\n",
      "Collecting build>=1.0.3\n",
      "  Downloading build-1.2.1-py3-none-any.whl (21 kB)\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Downloading bcrypt-4.1.2-cp39-abi3-win_amd64.whl (158 kB)\n",
      "     ------------------------------------ 158.3/158.3 kB 949.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.62.2)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (8.2.3)\n",
      "Collecting tokenizers>=0.13.2\n",
      "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 3.1 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
      "     -------------------------------------- 106.1/106.1 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (4.9.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (6.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (3.10.1)\n",
      "Collecting onnxruntime>=1.14.1\n",
      "  Downloading onnxruntime-1.17.3-cp310-cp310-win_amd64.whl (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 13.8 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.65.0\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "     -------------------------------------- 78.3/78.3 kB 618.7 kB/s eta 0:00:00\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
      "     --------------------------------------- 60.1/60.1 kB 26.8 kB/s eta 0:00:00\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
      "     -------------------------------------- 60.8/60.8 kB 648.4 kB/s eta 0:00:00\n",
      "Collecting typer>=0.9.0\n",
      "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "     --------------------------------------- 47.2/47.2 kB 40.1 kB/s eta 0:00:00\n",
      "Collecting posthog>=2.4.0\n",
      "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "     --------------------------------------- 41.3/41.3 kB 13.5 kB/s eta 0:00:00\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (1.23.5)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
      "Collecting fastapi>=0.95.2\n",
      "  Downloading fastapi-0.110.2-py3-none-any.whl (91 kB)\n",
      "     -------------------------------------- 91.9/91.9 kB 523.3 kB/s eta 0:00:00\n",
      "Collecting pypika>=0.48.9\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "     -------------------------------------- 67.3/67.3 kB 261.6 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting mmh3>=4.0.1\n",
      "  Downloading mmh3-4.1.0-cp310-cp310-win_amd64.whl (31 kB)\n",
      "Collecting chroma-hnswlib==0.7.3\n",
      "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-win_amd64.whl (150 kB)\n",
      "     -------------------------------------- 150.6/150.6 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting overrides>=7.3.1\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (2.28.1)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from chromadb) (2.7.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Collecting pyproject_hooks\n",
      "  Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "     -------------------------------------- 71.9/71.9 kB 171.3 kB/s eta 0:00:00\n",
      "Collecting requests-oauthlib\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2022.12.7)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.58.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.26.14)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.29.0)\n",
      "Collecting oauthlib>=3.2.2\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ------------------------------------ 151.7/151.7 kB 501.6 kB/s eta 0:00:00\n",
      "Collecting flatbuffers\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "     -------------------------------------- 46.0/46.0 kB 458.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
      "Collecting importlib-metadata<=7.0,>=6.0\n",
      "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-proto==1.24.0\n",
      "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
      "     -------------------------------------- 50.8/50.8 kB 259.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
      "Collecting opentelemetry-util-http==0.45b0\n",
      "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.45b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
      "Collecting opentelemetry-instrumentation==0.45b0\n",
      "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.45b0\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (65.6.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Collecting asgiref~=3.0\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting backoff>=1.10.0\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.28->chromadb) (2.0.4)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "     ------------------------------------ 388.9/388.9 kB 228.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.0.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (13.5.2)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Collecting watchfiles>=0.13\n",
      "  Downloading watchfiles-0.21.0-cp310-none-win_amd64.whl (279 kB)\n",
      "     ------------------------------------ 279.7/279.7 kB 297.4 kB/s eta 0:00:00\n",
      "Collecting websockets>=10.4\n",
      "  Downloading websockets-12.0-cp310-cp310-win_amd64.whl (124 kB)\n",
      "     ------------------------------------- 125.0/125.0 kB 68.0 kB/s eta 0:00:00\n",
      "Collecting httptools>=0.5.0\n",
      "  Downloading httptools-0.6.1-cp310-cp310-win_amd64.whl (58 kB)\n",
      "     -------------------------------------- 58.2/58.2 kB 153.6 kB/s eta 0:00:00\n",
      "Collecting python-dotenv>=0.13\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "     ------------------------------------ 172.0/172.0 kB 691.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.11.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (2.16.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (3.5.0)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "     -------------------------------------- 86.8/86.8 kB 695.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.2.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.95.2->chromadb) (1.2.0)\n",
      "Collecting pyreadline3\n",
      "  Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "     -------------------------------------- 95.2/95.2 kB 913.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml): started\n",
      "  Building wheel for pypika (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53835 sha256=506ab7eb4c6162b4778e844b2253581241ff24dae559aef4b3cbe8b5736f09d8\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\c4\\41\\b6\\f76a356f0791da799545c23894ceae842eeff054d0eb1fb626\n",
      "Successfully built pypika\n",
      "Installing collected packages: pyreadline3, pypika, monotonic, mmh3, flatbuffers, websockets, tqdm, shellingham, python-dotenv, pyproject_hooks, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, oauthlib, importlib-resources, importlib-metadata, humanfriendly, httptools, fsspec, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, requests-oauthlib, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, huggingface-hub, coloredlogs, build, typer, tokenizers, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.1\n",
      "    Uninstalling tqdm-4.64.1:\n",
      "      Successfully uninstalled tqdm-4.64.1\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.11.0\n",
      "    Uninstalling fsspec-2022.11.0:\n",
      "      Successfully uninstalled fsspec-2022.11.0\n",
      "  Attempting uninstall: bcrypt\n",
      "    Found existing installation: bcrypt 3.2.0\n",
      "    Uninstalling bcrypt-3.2.0:\n",
      "      Successfully uninstalled bcrypt-3.2.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.10.1\n",
      "    Uninstalling huggingface-hub-0.10.1:\n",
      "      Successfully uninstalled huggingface-hub-0.10.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.11.4\n",
      "    Uninstalling tokenizers-0.11.4:\n",
      "      Successfully uninstalled tokenizers-0.11.4\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.2 build-1.2.1 chroma-hnswlib-0.7.3 chromadb-0.5.0 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.110.2 flatbuffers-24.3.25 fsspec-2024.3.1 httptools-0.6.1 huggingface-hub-0.22.2 humanfriendly-10.0 importlib-metadata-7.0.0 importlib-resources-6.4.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.17.3 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 overrides-7.7.0 posthog-3.5.0 pypika-0.48.9 pyproject_hooks-1.0.0 pyreadline3-3.4.1 python-dotenv-1.0.1 requests-oauthlib-2.0.0 shellingham-1.5.4 starlette-0.37.2 tokenizers-0.19.1 tqdm-4.66.2 typer-0.12.3 uvicorn-0.29.0 watchfiles-0.21.0 websockets-12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.24.0 requires tokenizers!=0.11.3,<0.14,>=0.11.1, but you have tokenizers 0.19.1 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vGTUoeT9_4DG"
   },
   "outputs": [],
   "source": [
    "# Store the chunks in vector store\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Embed each chunk and load it into the vector store\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"Downloads\")\n",
    "\n",
    "# Persist the database on drive\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0OJbplIz_6mI"
   },
   "outputs": [],
   "source": [
    "# Setting a Connection with the ChromaDB\n",
    "connection = Chroma(persist_directory=\"Downloads\", embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNBM_Oou_8wL"
   },
   "source": [
    "# Settingup the Vector Store as a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "lZNYVHHVAAIk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.vectorstores.VectorStoreRetriever'>\n"
     ]
    }
   ],
   "source": [
    "# Converting CHROMA db_connection to Retriever Object\n",
    "retriever = connection.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "print(type(retriever))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APVneCHgADRh"
   },
   "source": [
    "Now let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJbGSaOGAGjE"
   },
   "source": [
    "# Based on users query retrieving the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ht0vmRFAHD0"
   },
   "source": [
    "### Query -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Pa_dITnBAMA-"
   },
   "outputs": [],
   "source": [
    "user_query = \"What is LLMs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bCBg4xoPAOgh"
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "V07YDjz-AQox"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "P0JlcHDkASzC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, the LLMs in their current state\n",
      "have yet to see an effective, practical compres-\n",
      "sive memory technique that balances simplicity along with quality.\n",
      "\n",
      "1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdFzaatYAU11"
   },
   "source": [
    "### Query - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_O7xLH45AYRO"
   },
   "outputs": [],
   "source": [
    "user_query = \"Tell me about LLMs?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "fbTrHU1kAaY6"
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "FtvxZLRvAcqA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NXbRwExAAekO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, the LLMs in their current state\n",
      "have yet to see an effective, practical compres-\n",
      "sive memory technique that balances simplicity along with quality.\n",
      "\n",
      "1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ka5JWzUrAg9z"
   },
   "source": [
    "# Passing the context and questioning to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4mdT1qsrAlJ4"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    # System Message Prompt Template\n",
    "    SystemMessage(content=\"\"\"You are a Helpful AI Bot.\n",
    "    You take the context and question from user. Your answer should be based on the specific context.\"\"\"),\n",
    "    # Human Message Prompt Template\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Answer the question based on the given context.\n",
    "    Context:\n",
    "    {context}\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer: \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rvURGOfGAntH"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "LbH6xqnfAqUQ"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | chat_template\n",
    "    | chat_model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3Kvze6HAuEe"
   },
   "source": [
    "## Query - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "pbxuulkTAzt0"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Scaled Dot-Product Attention Explained\n",
       "\n",
       "Unfortunately, the provided text doesn't explicitly define \"Scaled Dot-Product Attention.\" However, based on the context and references, we can infer some key points:\n",
       "\n",
       "**Dot-Product Attention:**\n",
       "\n",
       "* This is a fundamental attention mechanism where the attention weights are calculated using the dot product between query and key vectors. \n",
       "* The context mentions that it's used within each segment to compute the standard causal attention.\n",
       "* References like \"Efficient attention: Attention with linear complexities\" (Shen et al., 2018) likely delve deeper into the specifics of its calculation and efficiency improvements.\n",
       "\n",
       "**Scaling:**\n",
       "\n",
       "* The term \"scaled\" implies a normalization step is involved. This is typically done by dividing the dot product by the square root of the dimension of the key vectors. \n",
       "* This scaling helps to prevent the softmax function (often used in attention mechanisms) from having extremely small gradients, which can hinder training.\n",
       "\n",
       "**Contextual Clues:**\n",
       "\n",
       "* The paper contrasts Scaled Dot-Product Attention with \"local attention\" (Dai et al., 2019) which discards previous segment information. This suggests Scaled Dot-Product Attention might have a broader context window or a mechanism to incorporate information from previous segments.\n",
       "* The proposed \"Infini-attention\" is described as having minimal changes to the standard scaled dot-product attention. This further implies it's a well-known and established mechanism. \n",
       "\n",
       "**Further Exploration:**\n",
       "\n",
       "To fully understand Scaled Dot-Product Attention, I recommend:\n",
       "\n",
       "* **Consulting the referenced papers:** Especially \"Efficient attention: Attention with linear complexities\" (Shen et al., 2018) for a deeper understanding of its mechanics and efficiency considerations.\n",
       "* **Exploring resources on attention mechanisms:** Many online resources and textbooks provide detailed explanations of Scaled Dot-Product Attention and other attention variants.\n",
       "* **Examining the full paper:**  Once available, the full paper associated with this context will likely provide a more comprehensive explanation of Scaled Dot-Product Attention within the context of \"Infini-attention\" and its proposed advancements. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as markdown\n",
    "response = rag_chain.invoke(\"What is Scaled Dot-product Attention?\")\n",
    "\n",
    "markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdZ7nESvA5Jq"
   },
   "source": [
    "## Query - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "8wwygqOIA9Ha"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Memory and Effective Context Window: Balancing Power and Efficiency\n",
       "\n",
       "The provided text discusses the challenge of balancing memory requirements with the need for a large effective context window in LLMs (Large Language Models).  Here's a breakdown:\n",
       "\n",
       "**The Problem:**\n",
       "\n",
       "* LLMs benefit from access to a large context window, meaning they can \"remember\" and utilize information from a longer sequence of previous text.\n",
       "* However, storing and processing this information can be computationally expensive and memory-intensive, especially as the context window grows.\n",
       "\n",
       "**Existing Approaches:**\n",
       "\n",
       "The text mentions several approaches to address this challenge:\n",
       "\n",
       "* **Segment-level memory models:** These models divide the context into segments and selectively store and access them. However, they still face limitations in terms of efficiency and effectiveness.\n",
       "* **System-level optimization:** This involves leveraging specific hardware architectures to improve the efficiency of attention calculations, which are crucial for utilizing the context window.\n",
       "* **Memorizing Transformers:** This approach stores the entire Key-Value (KV) states as context but restricts contextual computation to a single layer to manage storage costs. It also utilizes a kNN retriever to build a context window covering the entire sequence history.\n",
       "\n",
       "**Challenges and Opportunities:**\n",
       "\n",
       "While these methods offer improvements, the text highlights the need for a more practical and efficient compressive memory technique. This technique should balance simplicity with the ability to maintain the quality of the LLM's output.\n",
       "\n",
       "**Overall, the quest for effectively managing memory and context window size remains an active area of research in the LLM domain.**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"Memory and Effective Context Window?\")\n",
    "\n",
    "markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
